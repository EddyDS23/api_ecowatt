# app/services/history_service.py
from sqlalchemy.orm import Session
from redis import Redis
from datetime import datetime, timedelta
from app.repositories import UserRepository
from app.core import logger
from app.schemas import HistoryPeriod
from collections import defaultdict

def get_history_data(db: Session, redis_client: Redis, user_id: int, period: HistoryPeriod):
    """
    Devuelve data_points agrupados para el periodo pedido:
     - daily  -> √∫ltimas 24h, bucket = 1 hora  (24 puntos)
     - weekly -> √∫ltimos 7d,  bucket = 1 d√≠a   (7 puntos)
     - monthly-> √∫ltimos 30d, bucket = 1 d√≠a   (30 puntos)
    """
    logger.info(f"üìä ============================================")
    logger.info(f"üìä SOLICITUD DE GR√ÅFICA: {period.value.upper()}")
    logger.info(f"üìä Usuario ID: {user_id}")
    logger.info(f"üìä ============================================")
    
    user_repo = UserRepository(db)
    user = user_repo.get_user_id_repository(user_id)
    if not user or not getattr(user, "devices", None):
        logger.warning(f"‚ùå Usuario {user_id} no encontrado o sin dispositivos")
        return None

    active_device = next((d for d in user.devices if d.dev_status), None)
    if not active_device:
        logger.warning(f"‚ùå Usuario {user_id} no tiene dispositivos activos")
        return None

    logger.info(f"‚úÖ Dispositivo activo: {active_device.dev_name} (ID: {active_device.dev_id})")
    
    watts_key = f"ts:user:{user_id}:device:{active_device.dev_id}:watts"
    logger.info(f"üîë Key de Redis: {watts_key}")

    # CR√çTICO: Usar hora LOCAL del servidor (CST)
    now_dt = datetime.now()
    now_ts = int(now_dt.timestamp() * 1000)

    # Configuraci√≥n de periodos y buckets
    if period == HistoryPeriod.DAILY:
        from_dt = now_dt - timedelta(hours=24)
        bucket_duration_ms = 60 * 60 * 1000  # 1 hora
        expected_buckets = 24
    elif period == HistoryPeriod.WEEKLY:
        from_dt = now_dt - timedelta(days=7)
        bucket_duration_ms = 24 * 60 * 60 * 1000  # 1 d√≠a
        expected_buckets = 7
    elif period == HistoryPeriod.MONTHLY:
        from_dt = now_dt - timedelta(days=30)
        bucket_duration_ms = 24 * 60 * 60 * 1000  # 1 d√≠a
        expected_buckets = 30
    else:
        logger.error(f"‚ùå Periodo no v√°lido: {period}")
        return None

    from_ts = int(from_dt.timestamp() * 1000)

    logger.info(f"‚è∞ Rango de consulta:")
    logger.info(f"   Desde: {from_dt} ({from_ts})")
    logger.info(f"   Hasta: {now_dt} ({now_ts})")
    logger.info(f"   Duraci√≥n bucket: {bucket_duration_ms / (60*60*1000):.1f} horas")
    logger.info(f"   Buckets esperados: {expected_buckets}")

    try:
        if not redis_client.exists(watts_key):
            logger.warning(f"‚ö†Ô∏è No existe la key en Redis: {watts_key}")
            return _generate_empty_response(period, from_ts, bucket_duration_ms, expected_buckets)

        if hasattr(redis_client, "ts"):
            return _get_data_with_timeseries(
                redis_client, watts_key, from_ts, now_ts,
                bucket_duration_ms, expected_buckets, period
            )
        else:
            return _get_data_with_zset_fallback(
                redis_client, watts_key, from_ts, now_ts,
                bucket_duration_ms, expected_buckets, period
            )
    except Exception as e:
        logger.error(f"‚ùå Error al obtener datos hist√≥ricos de {watts_key}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def _get_data_with_timeseries(redis_client, watts_key, from_ts, now_ts, bucket_duration_ms, expected_buckets, period):
    """Obtiene datos usando RedisTimeSeries con agregaci√≥n"""
    try:
        logger.info(f"üîç Consultando RedisTimeSeries...")
        
        aggregated_data = redis_client.ts().range(
            watts_key,
            from_time=from_ts,
            to_time=now_ts,
            aggregation_type="avg",
            bucket_size_msec=bucket_duration_ms
        )
        
        logger.info(f"üì• RedisTimeSeries devolvi√≥ {len(aggregated_data)} buckets con datos")

        if len(aggregated_data) == 0:
            logger.warning(f"‚ö†Ô∏è  No se encontraron datos en el rango especificado")
        else:
            logger.info(f"üìä Buckets recibidos de Redis:")
            for i, (ts, value) in enumerate(aggregated_data[:5]):
                dt = datetime.fromtimestamp(ts / 1000)
                logger.info(f"   [{i}] {dt} ‚Üí {value:.2f}W")
            if len(aggregated_data) > 5:
                logger.info(f"   ... y {len(aggregated_data) - 5} buckets m√°s")

        # Crear mapa de datos normalizados
        data_map = {}
        total_watts_sum = 0
        for ts, value in aggregated_data:
            normalized_ts = (ts // bucket_duration_ms) * bucket_duration_ms
            watts_value = float(value) if value is not None else 0.0
            data_map[normalized_ts] = watts_value
            total_watts_sum += watts_value

        avg_watts = total_watts_sum / len(aggregated_data) if aggregated_data else 0
        logger.info(f"üìà Promedio de Watts en periodo: {avg_watts:.2f}W")

        # Generar TODOS los buckets esperados
        # IMPORTANTE: Normalizar from_ts
        normalized_from_ts = (from_ts // bucket_duration_ms) * bucket_duration_ms
        current_ts = normalized_from_ts
        
        logger.info(f"üî® Generando {expected_buckets} data points...")
        logger.info(f"   Inicio normalizado: {datetime.fromtimestamp(normalized_from_ts / 1000)}")
        
        data_points = []
        total_kwh = 0
        points_with_data = 0
        
        for i in range(expected_buckets):
            bucket_start = current_ts
            dt_object = datetime.fromtimestamp(bucket_start / 1000)
            
            avg_power_watts = data_map.get(bucket_start, 0.0)
            bucket_hours = bucket_duration_ms / (1000 * 3600)
            kwh_value = (avg_power_watts * bucket_hours) / 1000.0
            
            data_points.append({
                "timestamp": dt_object,
                "value": round(kwh_value, 6)
            })
            
            if kwh_value > 0:
                points_with_data += 1
                total_kwh += kwh_value
                logger.info(f"   ‚úÖ Bucket [{i:2d}] {dt_object.strftime('%Y-%m-%d %H:%M')} ‚Üí {avg_power_watts:7.2f}W = {kwh_value:.6f} kWh")
            else:
                logger.debug(f"   ‚ö™ Bucket [{i:2d}] {dt_object.strftime('%Y-%m-%d %H:%M')} ‚Üí Sin datos (0 kWh)")
            
            current_ts += bucket_duration_ms

        logger.info(f"üìä ============================================")
        logger.info(f"üìä RESUMEN DE RESULTADOS")
        logger.info(f"üìä ============================================")
        logger.info(f"‚úÖ Total de puntos generados: {len(data_points)}")
        logger.info(f"üìà Puntos con datos: {points_with_data}")
        logger.info(f"üìâ Puntos vac√≠os: {len(data_points) - points_with_data}")
        logger.info(f"‚ö° Total kWh en periodo: {total_kwh:.4f} kWh")
        if points_with_data > 0:
            logger.info(f"‚ö° Promedio por punto: {total_kwh/points_with_data:.4f} kWh")
        else:
            logger.info(f"‚ö° Promedio: N/A")
        logger.info(f"üìä ============================================")
        
        return {"period": period.value, "data_points": data_points}

    except Exception as e:
        logger.error(f"‚ùå Error en _get_data_with_timeseries: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


def _get_data_with_zset_fallback(redis_client, watts_key, from_ts, now_ts, bucket_duration_ms, expected_buckets, period):
    """Fallback usando ZSET si no hay RedisTimeSeries"""
    try:
        raw = redis_client.zrangebyscore(watts_key, from_ts, now_ts, withscores=True)
        logger.info(f"   ZSET devolvi√≥ {len(raw)} puntos raw")

        buckets = {}
        current_ts = from_ts
        for _ in range(expected_buckets):
            buckets[current_ts] = {"sum": 0.0, "count": 0}
            current_ts += bucket_duration_ms

        for val, score in raw:
            try:
                v = float(val)
            except Exception:
                try:
                    import json
                    v = float(json.loads(val).get("watts", 0))
                except Exception:
                    v = 0.0
            bucket_start = ((int(score) - from_ts) // bucket_duration_ms) * bucket_duration_ms + from_ts
            if bucket_start in buckets:
                buckets[bucket_start]["sum"] += v
                buckets[bucket_start]["count"] += 1

        data_points = []
        for bucket_start in sorted(buckets.keys()):
            dt_object = datetime.fromtimestamp(bucket_start / 1000)
            agg = buckets[bucket_start]
            if agg["count"] == 0:
                kwh_value = 0.0
            else:
                avg_power_watts = agg["sum"] / agg["count"]
                bucket_hours = bucket_duration_ms / (1000 * 3600)
                kwh_value = (avg_power_watts * bucket_hours) / 1000.0
            data_points.append({"timestamp": dt_object, "value": round(kwh_value, 6)})

        logger.info(f"‚úÖ Generados {len(data_points)} puntos con ZSET para '{period.value}'")
        return {"period": period.value, "data_points": data_points}
    except Exception as e:
        logger.error(f"Error en _get_data_with_zset_fallback: {e}")
        raise


def _generate_empty_response(period, from_ts, bucket_duration_ms, expected_buckets):
    """Genera respuesta vac√≠a con todos los buckets en 0"""
    data_points = []
    current_ts = from_ts
    for _ in range(expected_buckets):
        dt_object = datetime.fromtimestamp(current_ts / 1000)
        data_points.append({"timestamp": dt_object, "value": 0.0})
        current_ts += bucket_duration_ms
    logger.info(f"‚ö†Ô∏è Generados {expected_buckets} puntos vac√≠os (sin datos en Redis)")
    return {"period": period.value, "data_points": data_points}


def get_last_7_days_data(db, redis_client, user_id: int):
    """
    Recupera datos de los √∫ltimos 7 d√≠as y devuelve data_points con kWh.
    GARANTIZA 7 PUNTOS COMPLETOS.
    """
    logger.info(f"üìä ============================================")
    logger.info(f"üìä SOLICITUD: √öLTIMOS 7 D√çAS")
    logger.info(f"üìä Usuario ID: {user_id}")
    logger.info(f"üìä ============================================")
    
    # Usar hora LOCAL del servidor (CST)
    now = datetime.now()
    start_time = (now - timedelta(days=7)).replace(hour=0, minute=0, second=0, microsecond=0)
    start_ts = int(start_time.timestamp() * 1000)
    end_ts = int(now.timestamp() * 1000)

    logger.info(f"‚è∞ Rango de consulta:")
    logger.info(f"   Desde: {start_time} ({start_ts})")
    logger.info(f"   Hasta: {now} ({end_ts})")

    keys = redis_client.keys(f"ts:user:{user_id}:device:*:watts")
    if not keys:
        logger.warning(f"‚ö†Ô∏è No se encontraron keys para user {user_id}")
        return _generate_empty_7_days_response(start_time)

    logger.info(f"‚úÖ Encontradas {len(keys)} keys de dispositivos")

    grouped_measurements = defaultdict(list)
    for key in keys:
        key_str = key.decode() if isinstance(key, bytes) else key
        logger.info(f"üîë Procesando: {key_str}")
        
        try:
            watts_data = redis_client.ts().range(key_str, start_ts, end_ts)
            logger.info(f"   üì• Puntos obtenidos: {len(watts_data)}")
            
            for ts, value in watts_data:
                dt = datetime.fromtimestamp(ts / 1000)
                date_str = dt.strftime("%Y-%m-%d")
                grouped_measurements[date_str].append({"timestamp": ts, "watts": float(value)})
        except Exception as e:
            logger.error(f"‚ùå Error procesando key {key_str}: {e}")
            continue

    logger.info(f"üìä Datos agrupados por fecha:")
    for date in sorted(grouped_measurements.keys()):
        logger.info(f"   {date}: {len(grouped_measurements[date])} mediciones")

    data_points = []
    total_kwh = 0
    days_with_data = 0
    
    for i in range(7):
        date_obj = start_time + timedelta(days=i)
        date_str = date_obj.strftime("%Y-%m-%d")
        timestamp_iso = date_obj.isoformat() + "Z"
        day_measurements = grouped_measurements.get(date_str, [])

        if not day_measurements:
            data_points.append({"timestamp": timestamp_iso, "value": 0.0})
            logger.info(f"   ‚ö™ {date_str}: Sin datos ‚Üí 0.0000 kWh")
        else:
            day_measurements.sort(key=lambda x: x["timestamp"])
            total_watt_seconds = 0.0
            for j in range(1, len(day_measurements)):
                t0 = day_measurements[j-1]["timestamp"]
                t1 = day_measurements[j]["timestamp"]
                w0 = day_measurements[j-1]["watts"]
                w1 = day_measurements[j]["watts"]
                dt_seconds = (t1 - t0) / 1000.0
                avg_watts = (w0 + w1) / 2.0
                total_watt_seconds += avg_watts * dt_seconds
            kwh_value = total_watt_seconds / 3_600_000.0
            data_points.append({"timestamp": timestamp_iso, "value": round(kwh_value, 4)})
            
            total_kwh += kwh_value
            days_with_data += 1
            
            avg_watts = sum(m["watts"] for m in day_measurements) / len(day_measurements)
            logger.info(f"   ‚úÖ {date_str}: {len(day_measurements)} mediciones ‚Üí Promedio {avg_watts:.2f}W = {kwh_value:.4f} kWh")

    logger.info(f"üìä ============================================")
    logger.info(f"üìä RESUMEN √öLTIMOS 7 D√çAS")
    logger.info(f"üìä ============================================")
    logger.info(f"‚úÖ Total de d√≠as: 7")
    logger.info(f"üìà D√≠as con datos: {days_with_data}")
    logger.info(f"üìâ D√≠as vac√≠os: {7 - days_with_data}")
    logger.info(f"‚ö° Total kWh: {total_kwh:.4f} kWh")
    if days_with_data > 0:
        logger.info(f"‚ö° Promedio diario: {total_kwh/days_with_data:.4f} kWh")
    else:
        logger.info(f"‚ö° Promedio: N/A")
    logger.info(f"üìä ============================================")

    return {"unit": "kWh", "data_points": data_points}


def _generate_empty_7_days_response(start_time):
    """Genera respuesta vac√≠a con 7 d√≠as de ceros"""
    data_points = []
    for i in range(7):
        date_obj = start_time + timedelta(days=i)
        timestamp_iso = date_obj.isoformat() + "Z"
        data_points.append({"timestamp": timestamp_iso, "value": 0.0})
    logger.info(f"‚ö†Ô∏è Generados 7 data_points vac√≠os (sin datos)")
    return {"unit": "kWh", "data_points": data_points}