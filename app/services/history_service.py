# app/services/history_service.py (VERSI√ìN CORREGIDA)
from sqlalchemy.orm import Session
from redis import Redis
from datetime import datetime, timezone, timedelta
from app.repositories import UserRepository
from app.core import logger
from app.schemas import HistoryPeriod
from collections import defaultdict

def get_history_data(db: Session, redis_client: Redis, user_id: int, period: HistoryPeriod):
    """
    Devuelve data_points agrupados para el periodo pedido:
     - daily  -> √∫ltimas 24h, bucket = 1 hora  (24 puntos GARANTIZADOS)
     - weekly -> √∫ltimos 7d,  bucket = 1 d√≠a   (7 puntos GARANTIZADOS)
     - monthly-> √∫ltimos 30d, bucket = 1 d√≠a   (30 puntos GARANTIZADOS)
    
    MODO DESARROLLO: Si hay pocos datos, ajusta autom√°ticamente el rango.
    """
    user_repo = UserRepository(db)
    user = user_repo.get_user_id_repository(user_id)
    if not user or not getattr(user, "devices", None):
        logger.warning(f"Usuario {user_id} no encontrado o sin dispositivos")
        return None

    active_device = next((d for d in user.devices if d.dev_status), None)
    if not active_device:
        logger.warning(f"Usuario {user_id} no tiene dispositivos activos")
        return None

    watts_key = f"ts:user:{user_id}:device:{active_device.dev_id}:watts"

    now_dt = datetime.now()
    now_ts = int(now_dt.timestamp() * 1000)

    # PASO 1: Detectar el rango real de datos disponibles
    try:
        if not redis_client.exists(watts_key):
            logger.warning(f"‚ö†Ô∏è No existe la key en Redis: {watts_key}")
            return _generate_empty_response(period, from_ts, bucket_duration_ms, expected_buckets)
        
        # Obtener primer y √∫ltimo timestamp
        all_data_sample = redis_client.ts().range(watts_key, "-", "+", count=1)
        if not all_data_sample:
            logger.warning(f"‚ö†Ô∏è La serie {watts_key} est√° vac√≠a")
            return _generate_empty_response(period, now_ts - 86400000, 3600000, 24)
        
        first_ts = all_data_sample[0][0]
        data_age_hours = (now_ts - first_ts) / (1000 * 3600)
        
        logger.info(f"üìä Datos disponibles desde hace {data_age_hours:.1f} horas")
        
    except Exception as e:
        logger.error(f"Error detectando rango de datos: {e}")
        data_age_hours = 24  # Asumir 24h por defecto

    # PASO 2: Configuraci√≥n adaptativa de periodos y buckets
    if period == HistoryPeriod.DAILY:
        # Si hay menos de 24h de datos, ajustar
        if data_age_hours < 24:
            # Usar el tiempo disponible, m√≠nimo 1 hora
            hours_available = max(1, int(data_age_hours))
            from_dt = now_dt - timedelta(hours=hours_available)
            bucket_duration_ms = 60 * 60 * 1000  # 1 hora
            expected_buckets = hours_available
            logger.warning(f"‚ö†Ô∏è Solo hay {hours_available}h de datos, ajustando gr√°fica diaria")
        else:
            from_dt = now_dt - timedelta(hours=24)
            bucket_duration_ms = 60 * 60 * 1000  # 1 hora
            expected_buckets = 24
            
    elif period == HistoryPeriod.WEEKLY:
        if data_age_hours < 168:  # 7 d√≠as = 168 horas
            days_available = max(1, int(data_age_hours / 24))
            from_dt = now_dt - timedelta(days=days_available)
            bucket_duration_ms = 24 * 60 * 60 * 1000  # 1 d√≠a
            expected_buckets = days_available
            logger.warning(f"‚ö†Ô∏è Solo hay {days_available} d√≠as de datos, ajustando gr√°fica semanal")
        else:
            from_dt = now_dt - timedelta(days=7)
            bucket_duration_ms = 24 * 60 * 60 * 1000  # 1 d√≠a
            expected_buckets = 7
            
    elif period == HistoryPeriod.MONTHLY:
        if data_age_hours < 720:  # 30 d√≠as = 720 horas
            days_available = max(1, int(data_age_hours / 24))
            from_dt = now_dt - timedelta(days=days_available)
            bucket_duration_ms = 24 * 60 * 60 * 1000  # 1 d√≠a
            expected_buckets = days_available
            logger.warning(f"‚ö†Ô∏è Solo hay {days_available} d√≠as de datos, ajustando gr√°fica mensual")
        else:
            from_dt = now_dt - timedelta(days=30)
            bucket_duration_ms = 24 * 60 * 60 * 1000  # 1 d√≠a
            expected_buckets = 30
    else:
        logger.error(f"Periodo no v√°lido: {period}")
        return None

    from_ts = int(from_dt.timestamp() * 1000)

    logger.info(f"üìä Obteniendo datos para periodo '{period.value}': {from_dt} ‚Üí {now_dt}")
    logger.info(f"   Bucket size: {bucket_duration_ms}ms, Expected buckets: {expected_buckets}")

    try:
        if not redis_client.exists(watts_key):
            logger.warning(f"‚ö†Ô∏è No existe la key en Redis: {watts_key}")
            return _generate_empty_response(period, from_ts, bucket_duration_ms, expected_buckets)

        # Verificar si tenemos RedisTimeSeries
        if hasattr(redis_client, "ts"):
            return _get_data_with_timeseries(
                redis_client, watts_key, from_ts, now_ts, 
                bucket_duration_ms, expected_buckets, period
            )
        else:
            return _get_data_with_zset_fallback(
                redis_client, watts_key, from_ts, now_ts, 
                bucket_duration_ms, expected_buckets, period
            )

    except Exception as e:
        logger.error(f"‚ùå Error al obtener datos hist√≥ricos de {watts_key}: {e}")
        return None


def _get_data_with_timeseries(redis_client, watts_key, from_ts, now_ts, bucket_duration_ms, expected_buckets, period):
    """Obtiene datos usando RedisTimeSeries con agregaci√≥n"""
    try:
        logger.info(f"üîç Consultando RedisTimeSeries...")
        
        # Obtener datos agregados de Redis
        aggregated_data = redis_client.ts().range(
            watts_key,
            from_time=from_ts,
            to_time=now_ts,
            aggregation_type="avg",
            bucket_size_msec=bucket_duration_ms
        )
        
        logger.info(f"üì• RedisTimeSeries devolvi√≥ {len(aggregated_data)} buckets con datos")

        if len(aggregated_data) == 0:
            logger.warning(f"‚ö†Ô∏è  No se encontraron datos en el rango especificado")
            logger.warning(f"   Esto puede significar que:")
            logger.warning(f"   1. El simulador no est√° enviando datos")
            logger.warning(f"   2. Los datos son demasiado antiguos")
            logger.warning(f"   3. Hay un problema de timezone")
        else:
            logger.info(f"üìä Buckets recibidos de Redis:")
            for i, (ts, value) in enumerate(aggregated_data[:5]):  # Mostrar primeros 5
                dt = datetime.fromtimestamp(ts / 1000)
                logger.info(f"   [{i}] {dt} ‚Üí {value:.2f}W")
            if len(aggregated_data) > 5:
                logger.info(f"   ... y {len(aggregated_data) - 5} buckets m√°s")

        # CR√çTICO: Crear un mapa normalizando los timestamps
        # Redis devuelve buckets alineados a la hora (ej: 20:00:00)
        # Necesitamos mapearlos correctamente
        data_map = {}
        total_watts_sum = 0
        for ts, value in aggregated_data:
            # Normalizar el timestamp del bucket al inicio de la hora/d√≠a
            normalized_ts = (ts // bucket_duration_ms) * bucket_duration_ms
            watts_value = float(value) if value is not None else 0.0
            data_map[normalized_ts] = watts_value
            total_watts_sum += watts_value
            
            dt = datetime.fromtimestamp(normalized_ts / 1000)
            logger.debug(f"   üîÑ Bucket normalizado: {dt} ‚Üí {watts_value:.2f}W")

        avg_watts = total_watts_sum / len(aggregated_data) if aggregated_data else 0
        logger.info(f"üìà Promedio de Watts en periodo: {avg_watts:.2f}W")

        # Generar TODOS los buckets esperados (incluyendo los vac√≠os)
        data_points = []
        
        # IMPORTANTE: Normalizar from_ts al inicio del bucket
        # Si from_ts = 21:22:45, normalizarlo a 21:00:00
        normalized_from_ts = (from_ts // bucket_duration_ms) * bucket_duration_ms
        current_ts = normalized_from_ts
        
        logger.info(f"üî® Generando {expected_buckets} data points...")
        logger.info(f"   Inicio normalizado: {datetime.fromtimestamp(normalized_from_ts / 1000)}")
        
        total_kwh = 0
        points_with_data = 0
        
        for i in range(expected_buckets):
            bucket_start = current_ts
            dt_object = datetime.fromtimestamp(bucket_start / 1000)
            
            # Buscar si hay datos para este bucket normalizado
            avg_power_watts = data_map.get(bucket_start, 0.0)
            
            # Convertir a kWh: Watts * horas / 1000
            bucket_hours = bucket_duration_ms / (1000 * 3600)
            kwh_value = (avg_power_watts * bucket_hours) / 1000.0
            
            data_points.append({
                "timestamp": dt_object,
                "value": round(kwh_value, 6)
            })
            
            if kwh_value > 0:
                points_with_data += 1
                total_kwh += kwh_value
                logger.info(f"   ‚úÖ Bucket [{i:2d}] {dt_object.strftime('%Y-%m-%d %H:%M')} ‚Üí {avg_power_watts:7.2f}W = {kwh_value:.6f} kWh")
            else:
                logger.debug(f"   ‚ö™ Bucket [{i:2d}] {dt_object.strftime('%Y-%m-%d %H:%M')} ‚Üí Sin datos (0 kWh)")
            
            current_ts += bucket_duration_ms

        logger.info(f"üìä ============================================")
        logger.info(f"üìä RESUMEN DE RESULTADOS")
        logger.info(f"üìä ============================================")
        logger.info(f"‚úÖ Total de puntos generados: {len(data_points)}")
        logger.info(f"üìà Puntos con datos: {points_with_data}")
        logger.info(f"üìâ Puntos vac√≠os: {len(data_points) - points_with_data}")
        logger.info(f"‚ö° Total kWh en periodo: {total_kwh:.4f} kWh")
        logger.info(f"‚ö° Promedio por punto: {total_kwh/points_with_data:.4f} kWh" if points_with_data > 0 else "‚ö° Promedio: N/A")
        logger.info(f"üìä ============================================")
        
        return {"period": period.value, "data_points": data_points}

    except Exception as e:
        logger.error(f"‚ùå Error en _get_data_with_timeseries: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


def _get_data_with_zset_fallback(redis_client, watts_key, from_ts, now_ts, bucket_duration_ms, expected_buckets, period):
    """Fallback usando ZSET si no hay RedisTimeSeries"""
    try:
        raw = redis_client.zrangebyscore(watts_key, from_ts, now_ts, withscores=True)
        logger.info(f"   ZSET devolvi√≥ {len(raw)} puntos raw")

        # Crear buckets vac√≠os
        buckets = {}
        current_ts = from_ts
        for i in range(expected_buckets):
            buckets[current_ts] = {"sum": 0.0, "count": 0}
            current_ts += bucket_duration_ms

        # Llenar buckets con datos reales
        for val, score in raw:
            try:
                v = float(val)
            except Exception:
                try:
                    import json
                    v = float(json.loads(val).get("watts", 0))
                except Exception:
                    v = 0.0
            
            # Encontrar el bucket correspondiente
            bucket_start = ((int(score) - from_ts) // bucket_duration_ms) * bucket_duration_ms + from_ts
            if bucket_start in buckets:
                buckets[bucket_start]["sum"] += v
                buckets[bucket_start]["count"] += 1

        # Generar data_points con todos los buckets
        data_points = []
        for bucket_start in sorted(buckets.keys()):
            dt_object = datetime.fromtimestamp(bucket_start / 1000)
            agg = buckets[bucket_start]
            
            if agg["count"] == 0:
                kwh_value = 0.0
            else:
                avg_power_watts = agg["sum"] / agg["count"]
                bucket_hours = bucket_duration_ms / (1000 * 3600)
                kwh_value = (avg_power_watts * bucket_hours) / 1000.0
            
            data_points.append({
                "timestamp": dt_object,
                "value": round(kwh_value, 6)
            })

        logger.info(f"‚úÖ Generados {len(data_points)} puntos con ZSET para '{period.value}'")
        return {"period": period.value, "data_points": data_points}

    except Exception as e:
        logger.error(f"Error en _get_data_with_zset_fallback: {e}")
        raise


def _generate_empty_response(period, from_ts, bucket_duration_ms, expected_buckets):
    """Genera una respuesta con buckets vac√≠os cuando no hay datos"""
    data_points = []
    current_ts = from_ts
    
    for i in range(expected_buckets):
        dt_object = datetime.fromtimestamp(current_ts / 1000)
        data_points.append({
            "timestamp": dt_object,
            "value": 0.0
        })
        current_ts += bucket_duration_ms
    
    logger.info(f"‚ö†Ô∏è Generados {expected_buckets} puntos vac√≠os (sin datos en Redis)")
    return {"period": period.value, "data_points": data_points}


def get_last_7_days_data(db, redis_client, user_id: int):
    """
    Recupera datos de los √∫ltimos 7 d√≠as desde RedisTimeSeries y devuelve
    data_points con timestamp y value en kWh.
    GARANTIZA 7 PUNTOS COMPLETOS.
    
    Formato de respuesta:
    {
        "unit": "kWh",
        "data_points": [
            {"timestamp": "2025-10-19T00:00:00Z", "value": 0.5},
            {"timestamp": "2025-10-20T00:00:00Z", "value": 0.8},
            ...
        ]
    }
    """
    now = datetime.now()
    # Calcular inicio hace exactamente 7 d√≠as (a medianoche)
    start_time = (now - timedelta(days=7)).replace(hour=0, minute=0, second=0, microsecond=0)
    start_ts = int(start_time.timestamp() * 1000)
    end_ts = int(now.timestamp() * 1000)

    logger.info(f"üìä get_last_7_days_data para user {user_id}")
    logger.info(f"   Rango: {start_time} ‚Üí {now}")

    # Buscar todas las keys de dispositivos del usuario
    keys = redis_client.keys(f"ts:user:{user_id}:device:*:watts")
    if not keys:
        logger.warning(f"‚ö†Ô∏è No se encontraron keys para user {user_id}")
        return _generate_empty_7_days_response(start_time)

    logger.info(f"   Encontradas {len(keys)} keys de dispositivos")

    # Diccionario para agrupar TODAS las mediciones por fecha
    grouped_measurements = defaultdict(list)

    # Procesar cada dispositivo
    for key in keys:
        key_str = key.decode() if isinstance(key, bytes) else key
        logger.info(f"   Procesando key: {key_str}")

        try:
            # Obtener TODOS los puntos de watts en el rango
            watts_data = redis_client.ts().range(key_str, start_ts, end_ts)
            logger.info(f"      Watts: {len(watts_data)} puntos")

            # Agrupar TODAS las mediciones por fecha
            for ts, value in watts_data:
                try:
                    # Convertir timestamp a fecha (sin hora)
                    dt = datetime.fromtimestamp(ts / 1000)
                    date_str = dt.strftime("%Y-%m-%d")
                    
                    # Guardar timestamp y valor
                    grouped_measurements[date_str].append({
                        "timestamp": ts,
                        "watts": float(value)
                    })
                except Exception as e:
                    logger.error(f"Error procesando punto: {e}")
                    continue

        except Exception as e:
            logger.error(f"‚ùå Error procesando key {key_str}: {e}")
            continue

    # Log de datos agrupados
    logger.info(f"   Datos agrupados por fecha: {list(grouped_measurements.keys())}")
    for date, measurements in grouped_measurements.items():
        logger.info(f"      {date}: {len(measurements)} mediciones")

    # Generar data_points para los √∫ltimos 7 d√≠as
    data_points = []
    
    for i in range(7):
        # Fecha del d√≠a
        date_obj = start_time + timedelta(days=i)
        date_str = date_obj.strftime("%Y-%m-%d")
        
        # Timestamp ISO 8601 a medianoche de ese d√≠a
        timestamp_iso = date_obj.replace(hour=0, minute=0, second=0, microsecond=0).isoformat() + "Z"
        
        # Obtener mediciones de ese d√≠a
        day_measurements = grouped_measurements.get(date_str, [])
        
        if not day_measurements:
            # D√≠a sin datos
            data_points.append({
                "timestamp": timestamp_iso,
                "value": 0.0
            })
            logger.info(f"      {date_str}: Sin datos ‚Üí 0.0 kWh")
        else:
            # Calcular kWh del d√≠a usando integraci√≥n trapezoidal
            # Ordenar por timestamp
            day_measurements.sort(key=lambda x: x["timestamp"])
            
            total_watt_seconds = 0.0
            for j in range(1, len(day_measurements)):
                t0 = day_measurements[j-1]["timestamp"]
                t1 = day_measurements[j]["timestamp"]
                w0 = day_measurements[j-1]["watts"]
                w1 = day_measurements[j]["watts"]
                
                # Tiempo transcurrido en segundos
                dt_seconds = (t1 - t0) / 1000.0
                
                # Potencia promedio en el intervalo
                avg_watts = (w0 + w1) / 2.0
                
                # Energ√≠a = Potencia √ó Tiempo
                total_watt_seconds += avg_watts * dt_seconds
            
            # Convertir Watt-segundos a kWh
            kwh_value = total_watt_seconds / 3_600_000.0  # (W¬∑s) / (1000 W/kW √ó 3600 s/h)
            
            data_points.append({
                "timestamp": timestamp_iso,
                "value": round(kwh_value, 4)
            })
            logger.info(f"      {date_str}: {len(day_measurements)} mediciones ‚Üí {kwh_value:.4f} kWh")

    logger.info(f"‚úÖ get_last_7_days_data: Generados {len(data_points)} data_points")
    
    return {
        "unit": "kWh",
        "data_points": data_points
    }


def _generate_empty_7_days_response(start_time):
    """Genera respuesta vac√≠a con 7 d√≠as de ceros en formato data_points"""
    data_points = []
    
    for i in range(7):
        date_obj = start_time + timedelta(days=i)
        timestamp_iso = date_obj.replace(hour=0, minute=0, second=0, microsecond=0).isoformat() + "Z"
        
        data_points.append({
            "timestamp": timestamp_iso,
            "value": 0.0
        })
    
    logger.info(f"‚ö†Ô∏è Generados 7 data_points vac√≠os (sin datos)")
    return {
        "unit": "kWh",
        "data_points": data_points
    }